{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",      # transport: stdio, http, websocket based on MCP Server\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='c2bc8ef2-5cd0-480b-92e9-ecf8e2573501'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 271, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D41kwzhGU6eMwhZ5NNK6bPkQpjNjM', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c136c-f51e-7a90-8f4a-927854a9b8ff-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_fZaTFUAi9X9PYkmpmTVzJqrn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 156, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.99999535,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub\",\\n      \"content\": \"This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.\",\\n      \"score\": 0.9999906,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two\",\\n      \"score\": 0.9999863,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca\",\\n      \"title\": \"The Complete Guide to langchain-mcp-adapters - Medium\",\\n      \"content\": \"This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"] }, \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\\\\\"claude-sonnet-4-5-20250929\\\\\", tools)# Use the agentresponse = await agent.ainvoke({ \\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\\\\\"openai:gpt-4.1\\\\\")client = MultiServerMCPClient({ \\\\\"math\\\\\": { \\\\\"command\\\\\": \\\\\"python\\\\\", \\\\\"args\\\\\": [\\\\\"./examples/math_server.py\\\\\"], \\\\\"transport\\\\\": \\\\\"stdio\\\\\" }, \\\\\"weather\\\\\": { \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\\\\\"messages\\\\\"]) return {\\\\\"messages\\\\\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \\\\\"call_model\\\\\")builder.add_conditional_edges(\\\\\"call_model\\\\\", tools_condition)builder.add_edge(\\\\\"tools\\\\\", \\\\\"call_model\\\\\").\",\\n      \"score\": 0.99998605,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.99997294,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.57,\\n  \"request_id\": \"b0237832-9694-46da-9e57-ee988a36ce0f\"\\n}', 'id': 'lc_4f03145e-52fc-4d57-a908-fb2414bb1799'}], name='search_web', id='5409051c-cbfa-4396-b559-0bbfcebc6d43', tool_call_id='call_fZaTFUAi9X9PYkmpmTVzJqrn', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.99999535, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub', 'content': 'This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.', 'score': 0.9999906, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two', 'score': 0.9999863, 'raw_content': None}, {'url': 'https://medium.com/@deepakkamboj/the-complete-guide-to-langchain-mcp-adapters-bridging-langchain-and-model-context-protocol-3f5507cbd3ca', 'title': 'The Complete Guide to langchain-mcp-adapters - Medium', 'content': 'This open-source package eliminates the complexity of manual tool integration by providing seamless conversion between MCP tools and LangChain-compatible tools, enabling developers to tap into hundreds of existing MCP servers without writing custom adapters. from langchain_mcp_adapters.client import MultiServerMCPClientfrom langchain.agents import create_agentclient = MultiServerMCPClient({ \"math\": { \"transport\": \"stdio\", \"command\": \"python\", \"args\": [\"/path/to/math_server.py\"] }, \"weather\": { \"transport\": \"streamable_http\", \"url\": \"http://localhost:8000/mcp\" }})# Load all tools from all serverstools = await client.get_tools()# Create an agent with these toolsagent = create_agent(\"claude-sonnet-4-5-20250929\", tools)# Use the agentresponse = await agent.ainvoke({ \"messages\": [{\"role\": \"user\", \"content\": \"what\\'s (3 + 5) x 12?\"}]}). from langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom langchain.chat_models import init_chat_modelmodel = init_chat_model(\"openai:gpt-4.1\")client = MultiServerMCPClient({ \"math\": { \"command\": \"python\", \"args\": [\"./examples/math_server.py\"], \"transport\": \"stdio\" }, \"weather\": { \"url\": \"http://localhost:8000/mcp\", \"transport\": \"streamable_http\" }})tools = await client.get_tools()def call_model(state: MessagesState): response = model.bind_tools(tools).invoke(state[\"messages\"]) return {\"messages\": response}builder = StateGraph(MessagesState)builder.add_node(call_model)builder.add_node(ToolNode(tools))builder.add_edge(START, \"call_model\")builder.add_conditional_edges(\"call_model\", tools_condition)builder.add_edge(\"tools\", \"call_model\").', 'score': 0.99998605, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.99997294, 'raw_content': None}], 'response_time': 0.57, 'request_id': 'b0237832-9694-46da-9e57-ee988a36ce0f'}}}),\n",
      "              AIMessage(content='Here’s a concise overview of the langchain-mcp-adapters library.\\n\\nWhat it is\\n- A wrapper library that lets you use Anthropic’s Model Context Protocol (MCP) tools inside LangChain and LangGraph.\\n- It converts MCP tools into LangChain- and LangGraph-compatible tools, so you can orchestrate and reason with them like native LangChain tools.\\n- It’s designed to let you interact with tools from multiple MCP servers at once and to plug those tools into LangGraph agents.\\n\\nWhy you’d use it\\n- You can leverage the large MCP ecosystem (hundreds of published tool servers) without writing custom adapters for each tool.\\n- It enables cross-server tool usage, so an agent can pull in tools from multiple MCP servers to solve a task.\\n- It provides a smoother integration path between MCP tools and LangChain/LangGraph tooling.\\n\\nKey features (as described in official sources)\\n- Converts MCP tools to LangChain-compatible tools and handles the conversion/compatibility layer.\\n- Supports interaction with tools across multiple MCP servers.\\n- Integrates MCP tools into LangGraph agents, enabling graph-based tool use and orchestration.\\n- Helps you load and manage tools from many MCP servers with configurable behavior (timeouts, auth, etc.).\\n\\nTypical usage patterns\\n- Python (LangChain ecosystem):\\n  - Install: pip install langchain-mcp-adapters\\n  - Create a client that connects to one or more MCP servers, using transports such as stdio (local processes) or HTTP-based streams.\\n  - Retrieve tools from the MCP servers and convert them into LangChain-compatible tools.\\n  - Use those tools to build an agent (e.g., with LangChain agents) and invoke it with user messages.\\n- Node/JavaScript (LangChain for JS):\\n  - Install: npm install @langchain/mcp-adapters\\n  - Similar concept: connect to MCP servers, load tools, and expose them as LangChain-compatible tools for use in agents or workflows.\\n\\nNotes and context\\n- MCP Adapters are designed to simplify integrating the MCP ecosystem with LangChain and LangGraph, removing the need to manually adapt each tool.\\n- They support multi-server setups, so you can compose tools across different MCP servers for richer capabilities.\\n- Examples and documentation illustrate both Python and JavaScript usage, and there are practical guides showing how to load tools and wire them into agents.\\n\\nWhere to read more (reference sources)\\n- LangChain changelog: MCP Adapters for LangChain and LangGraph — overview of what the adapters do and why they’re useful.\\n- GitHub: langchain-ai/langchain-mcp-adapters — official repo with library details and usage notes.\\n- LangChain docs: langchain_mcp_adapters module — high-level description of the adapter’s functionality and migration guidance.\\n- Medium guide: The Complete Guide to langchain-mcp-adapters — practical, end-to-end examples (Python) showing how to connect MCP servers, load tools, and run an agent.\\n- NPM package: @langchain/mcp-adapters — Node.js/JS implementation and usage notes for connecting to MCP servers and loading tools.\\n\\nWould you like me to:\\n- Provide a quick start guide with concrete Python or JavaScript code samples?\\n- Walk you through wiring up a simple example (e.g., a local MCP server and a LangChain agent)?\\n- Point you to a specific repo/docs page and pull the exact installation and code snippets?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2238, 'prompt_tokens': 1625, 'total_tokens': 3863, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1536, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D41l1TNekguJUfA8FLSw4fQEcvOb0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c136d-07f2-7753-b931-6c174a31eab0-0', usage_metadata={'input_tokens': 1625, 'output_tokens': 2238, 'total_tokens': 3863, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1536}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it in Vietnam?', additional_kwargs={}, response_metadata={}, id='3f10523b-a9a5-4451-bbec-7ce0fbec9a97'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 286, 'prompt_tokens': 298, 'total_tokens': 584, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D42GdSmGPTbDJ0EnLWEadJbalNdRN', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c138a-f114-7ce1-add6-b781dcd7722c-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Asia/Ho_Chi_Minh'}, 'id': 'call_fA2FNAxVzOH76ZosMbTglxtq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 298, 'output_tokens': 286, 'total_tokens': 584, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Asia/Ho_Chi_Minh\",\\n  \"datetime\": \"2026-01-31T17:13:22+07:00\",\\n  \"day_of_week\": \"Saturday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_9240bcd5-031e-4d81-93cc-b2672cd5510c'}], name='get_current_time', id='e2425521-40d1-4c01-b6ce-d4a21db1bc13', tool_call_id='call_fA2FNAxVzOH76ZosMbTglxtq'),\n",
      "              AIMessage(content='In Vietnam (ICT, UTC+7), it’s 17:13 (5:13 PM) on Saturday, January 31, 2026. Vietnam does not observe daylight saving time.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 433, 'prompt_tokens': 387, 'total_tokens': 820, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D42GgypBYToSDwGIDViOx9fKk8Eoa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c138a-ff67-7a92-ac17-20edd259db36-0', usage_metadata={'input_tokens': 387, 'output_tokens': 433, 'total_tokens': 820, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it in Vietnam?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnam (ICT, UTC+7) is currently Saturday, January 31, 2026, 17:11 (5:11 PM).\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4064d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
